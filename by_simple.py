#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­—èŠ‚è·³åŠ¨èŒä½ç›‘æ§ - ç®€åŒ–ç‰ˆæœ¬ï¼ˆæ— Playwrightä¾èµ–ï¼‰
é€‚ç”¨äºäº‘å¹³å°éƒ¨ç½²ï¼Œä½¿ç”¨requests + æ¨¡æ‹Ÿè¯·æ±‚æ–¹å¼
"""

import json
import logging
import requests
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd
import openpyxl
from openpyxl.styles import PatternFill

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)

# æ–‡ä»¶è·¯å¾„é…ç½®
PROJECT_PATH = Path(__file__).parent
DATA_PATH = PROJECT_PATH / "data"
DATA_PATH.mkdir(exist_ok=True)
OUTPUT_FILENAME = DATA_PATH / "bytedance_jobs_tracker.xlsx"
JSON_CACHE_FILENAME = DATA_PATH / "bytedance_jobs_cache.json"

# ä»»åŠ¡é…ç½® - ç›´æ¥ä½¿ç”¨APIæ¥å£
TASK_CONFIGS: List[Dict[str, Any]] = [
    {
        'id': 1,
        'name': 'å®ä¹ æ‹›è˜',
        'sheet_name': 'intern',
        'api_url': 'https://jobs.bytedance.com/api/v1/search/job/posts',
        'params': {
            'keywords': '',
            'category': '6704215864629004552,6704215864591255820,6704216224387041544,6704215924712409352',
            'location': 'CT_125',
            'project': '7481474995534301447,7468181472685164808,7194661644654577981,7194661126919358757',
            'type': '',
            'job_hot_flag': '',
            'current': 1,
            'limit': 2000,
            'functionCategory': '',
            'tag': ''
        }
    },
    {
        'id': 2,
        'name': 'æ ¡å›­æ‹›è˜',
        'sheet_name': 'campus',
        'api_url': 'https://jobs.bytedance.com/api/v1/search/job/posts',
        'params': {
            'keywords': '',
            'category': '6704215864629004552,6704215864591255820,6704216224387041544,6704215924712409352',
            'location': 'CT_125',
            'project': '7525009396952582407',
            'type': '',
            'job_hot_flag': '',
            'current': 1,
            'limit': 2000,
            'functionCategory': '',
            'tag': ''
        }
    },
    {
        'id': 3,
        'name': 'ç¤¾ä¼šæ‹›è˜',
        'sheet_name': 'experienced',
        'api_url': 'https://jobs.bytedance.com/api/v1/search/job/posts',
        'params': {
            'keywords': '',
            'category': '6704215864629004552,6704215864591255820,6704215924712409352,6704216224387041544',
            'location': 'CT_125',
            'project': '',
            'type': '',
            'job_hot_flag': '',
            'current': 1,
            'limit': 600,
            'functionCategory': '',
            'tag': ''
        }
    }
]

class SimpleJobMonitor:
    """ç®€åŒ–ç‰ˆèŒä½ç›‘æ§å™¨ - ä¸ä¾èµ–Playwright"""
    
    def __init__(self, tasks: List[Dict[str, Any]], filename: Path):
        self.tasks = tasks
        self.filename = filename
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://jobs.bytedance.com/',
            'Origin': 'https://jobs.bytedance.com',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
        })
    
    def fetch_jobs(self, task_config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·å–èŒä½æ•°æ® - å¸¦é‡è¯•æœºåˆ¶"""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    logging.info(f"é‡è¯•è·å– {task_config['name']} æ•°æ® (ç¬¬{attempt+1}æ¬¡)...")
                else:
                    logging.info(f"æ­£åœ¨è·å– {task_config['name']} æ•°æ®...")
                
                # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                if attempt > 0:
                    import time
                    time.sleep(retry_delay * attempt)
                
                # è°ƒè¯•ï¼šæ‰“å°è¯·æ±‚ä¿¡æ¯
                full_url = f"{task_config['api_url']}?{'&'.join([f'{k}={v}' for k, v in task_config['params'].items()])}"
                logging.info(f"è¯·æ±‚URL: {full_url}")
                
                response = self.session.get(
                    task_config['api_url'],
                    params=task_config['params'],
                    timeout=30
                )
                
                # è°ƒè¯•ï¼šæ‰“å°å“åº”ä¿¡æ¯
                logging.info(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                logging.info(f"å“åº”å¤´: {dict(response.headers)}")
                logging.info(f"å“åº”å†…å®¹å‰200å­—ç¬¦: {response.text[:200]}")
                
                if response.status_code == 200:
                    try:
                        data = response.json()
                        
                        if 'data' in data and 'job_post_list' in data['data']:
                            jobs = data['data']['job_post_list']
                            logging.info(f"âœ… {task_config['name']} è·å–æˆåŠŸ: {len(jobs)} ä¸ªèŒä½")
                            return jobs
                        elif 'data' in data and isinstance(data['data'], list):
                            # å…¼å®¹ä¸åŒçš„APIå“åº”æ ¼å¼
                            jobs = data['data']
                            logging.info(f"âœ… {task_config['name']} è·å–æˆåŠŸ: {len(jobs)} ä¸ªèŒä½")
                            return jobs
                        else:
                            logging.warning(f"âš ï¸ {task_config['name']} æ•°æ®æ ¼å¼å¼‚å¸¸: {list(data.keys()) if isinstance(data, dict) else type(data)}")
                            if attempt == max_retries - 1:
                                return []
                            continue
                    except json.JSONDecodeError as e:
                        logging.error(f"âŒ {task_config['name']} JSONè§£æå¤±è´¥: {e}")
                        if attempt == max_retries - 1:
                            return []
                        continue
                        
                elif response.status_code == 429:
                    # è¯·æ±‚é¢‘ç‡é™åˆ¶
                    logging.warning(f"âš ï¸ {task_config['name']} è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…é‡è¯•...")
                    if attempt < max_retries - 1:
                        import time
                        time.sleep(10)  # ç­‰å¾…æ›´é•¿æ—¶é—´
                        continue
                    else:
                        logging.error(f"âŒ {task_config['name']} è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œé‡è¯•å¤±è´¥")
                        return []
                        
                elif response.status_code in [403, 404]:
                    # æƒé™æˆ–èµ„æºä¸å­˜åœ¨é”™è¯¯ï¼Œä¸é‡è¯•
                    logging.error(f"âŒ {task_config['name']} è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text[:200]}")
                    return []
                    
                else:
                    logging.error(f"âŒ {task_config['name']} è¯·æ±‚å¤±è´¥: {response.status_code}")
                    if attempt == max_retries - 1:
                        return []
                    continue
                    
            except requests.exceptions.Timeout:
                logging.warning(f"âš ï¸ {task_config['name']} è¯·æ±‚è¶…æ—¶")
                if attempt == max_retries - 1:
                    logging.error(f"âŒ {task_config['name']} å¤šæ¬¡è¶…æ—¶ï¼Œè·å–å¤±è´¥")
                    return []
                continue
                
            except requests.exceptions.ConnectionError:
                logging.warning(f"âš ï¸ {task_config['name']} è¿æ¥é”™è¯¯")
                if attempt == max_retries - 1:
                    logging.error(f"âŒ {task_config['name']} è¿æ¥å¤±è´¥")
                    return []
                continue
                
            except Exception as e:
                logging.error(f"âŒ {task_config['name']} è·å–å¤±è´¥: {e}")
                if attempt == max_retries - 1:
                    return []
                continue
        
        return []
    
    def process_job_data(self, jobs: List[Dict[str, Any]]) -> pd.DataFrame:
        """å¤„ç†èŒä½æ•°æ®"""
        processed_jobs = []
        
        for job in jobs:
            try:
                processed_job = {
                    'èŒä½åç§°': job.get('title', ''),
                    'éƒ¨é—¨': job.get('department', ''),
                    'å·¥ä½œåœ°ç‚¹': job.get('city_info', {}).get('city_name', ''),
                    'å‘å¸ƒæ—¶é—´': job.get('publish_time', ''),
                    'æ›´æ–°æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'èŒä½ID': job.get('id', ''),
                    'èŒä½é“¾æ¥': f"https://jobs.bytedance.com/campus/position/{job.get('id', '')}",
                    'å·¥ä½œæ€§è´¨': job.get('job_type', {}).get('name', ''),
                    'å­¦å†è¦æ±‚': job.get('requirement', ''),
                    'èŒä½æè¿°': job.get('description', '')[:500] + '...' if len(job.get('description', '')) > 500 else job.get('description', '')
                }
                processed_jobs.append(processed_job)
            except Exception as e:
                logging.warning(f"å¤„ç†èŒä½æ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        return pd.DataFrame(processed_jobs)
    
    def save_to_excel(self, data_frames: Dict[str, pd.DataFrame]):
        """ä¿å­˜åˆ°Excelæ–‡ä»¶"""
        try:
            with pd.ExcelWriter(self.filename, engine='openpyxl') as writer:
                for sheet_name, df in data_frames.items():
                    if not df.empty:
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                        
                        # è®¾ç½®åˆ—å®½
                        worksheet = writer.sheets[sheet_name]
                        for column in worksheet.columns:
                            max_length = 0
                            column_letter = column[0].column_letter
                            for cell in column:
                                try:
                                    if len(str(cell.value)) > max_length:
                                        max_length = len(str(cell.value))
                                except:
                                    pass
                            adjusted_width = min(max_length + 2, 50)
                            worksheet.column_dimensions[column_letter].width = adjusted_width
            
            logging.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {self.filename}")
            
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
    
    def save_json_cache(self, data_frames: Dict[str, pd.DataFrame]):
        """ä¿å­˜JSONç¼“å­˜"""
        try:
            cache_data = {}
            for sheet_name, df in data_frames.items():
                if not df.empty:
                    cache_data[sheet_name] = df.to_dict('records')
            
            with open(JSON_CACHE_FILENAME, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            logging.info(f"âœ… JSONç¼“å­˜å·²ä¿å­˜åˆ°: {JSON_CACHE_FILENAME}")
            
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜JSONç¼“å­˜å¤±è´¥: {e}")
    
    def run(self, silent_mode: bool = False):
        """è¿è¡Œç›‘æ§ä»»åŠ¡"""
        logging.info("ğŸš€ å¼€å§‹æ‰§è¡Œå­—èŠ‚è·³åŠ¨èŒä½ç›‘æ§ä»»åŠ¡ - ç®€åŒ–ç‰ˆæœ¬")
        
        data_frames = {}
        total_jobs = 0
        
        for task_config in self.tasks:
            jobs = self.fetch_jobs(task_config)
            
            if jobs:
                df = self.process_job_data(jobs)
                data_frames[task_config['sheet_name']] = df
                total_jobs += len(jobs)
            else:
                # åˆ›å»ºç©ºçš„DataFrame
                data_frames[task_config['sheet_name']] = pd.DataFrame()
        
        if data_frames:
            self.save_to_excel(data_frames)
            self.save_json_cache(data_frames)
        
        logging.info(f"âœ… ä»»åŠ¡å®Œæˆ! å…±è·å– {total_jobs} ä¸ªèŒä½")
        
        if not silent_mode:
            try:
                # å°è¯•å‘é€æ¡Œé¢é€šçŸ¥
                import subprocess
                subprocess.run([
                    'osascript', '-e',
                    f'display notification "å…±è·å– {total_jobs} ä¸ªèŒä½" with title "å­—èŠ‚è·³åŠ¨èŒä½ç›‘æ§"'
                ], check=False)
            except:
                pass
        
        return {
            'success': True,
            'total_jobs': total_jobs,
            'tasks_completed': len(self.tasks),
            'message': f'æˆåŠŸè·å– {total_jobs} ä¸ªèŒä½'
        }

if __name__ == "__main__":
    try:
        is_silent = "--auto" in sys.argv
        monitor = SimpleJobMonitor(tasks=TASK_CONFIGS, filename=OUTPUT_FILENAME)
        result = monitor.run(silent_mode=is_silent)
        
        if result['success']:
            logging.info("ğŸ‰ ç¨‹åºæ‰§è¡ŒæˆåŠŸ!")
        else:
            logging.error("âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logging.info("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
        sys.exit(0)
    except Exception as e:
        logging.critical(f"\nâŒ ç¨‹åºå‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)