# bytedance_job_monitor.py

import asyncio
import hashlib
import logging
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Any

import openpyxl
import pandas as pd
from openpyxl.styles import PatternFill
from playwright.async_api import async_playwright, Browser

# --- 1. é…ç½®åŒº ---

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)

# æ–‡ä»¶ååŠè·¯å¾„é…ç½®
# æ–‡ä»¶å°†ä¿å­˜åœ¨å½“å‰ç”¨æˆ·çš„â€œæ–‡ç¨¿â€ (Documents) æ–‡ä»¶å¤¹ä¸‹
DOCUMENTS_PATH = Path.home() / "Documents"
OUTPUT_FILENAME = DOCUMENTS_PATH / "bytedance_jobs_tracker.xlsx"

# ä»»åŠ¡é…ç½®
TASK_CONFIGS: List[Dict[str, Any]] = [
    {
        'id': 1,
        'name': 'å®ä¹ æ‹›è˜',
        'sheet_name': 'intern',
        'url': "https://jobs.bytedance.com/campus/position?keywords=&category=6704215864629004552%2C6704215864591255820%2C6704216224387041544%2C6704215924712409352&location=CT_125&project=7481474995534301447%2C7468181472685164808%2C7194661644654577981%2C7194661126919358757&type=&job_hot_flag=&current=1&limit=2000&functionCategory=&tag=",
        'api_url_mark': "api/v1/search/job/posts",
        'extra_fields': []
    },
    {
        'id': 2,
        'name': 'æ ¡å›­æ‹›è˜',
        'sheet_name': 'campus',
        'url': "https://jobs.bytedance.com/campus/position?keywords=&category=6704215864629004552%2C6704215864591255820%2C6704216224387041544%2C6704215924712409352&location=CT_125&project=7525009396952582407&type=&job_hot_flag=&current=1&limit=2000&functionCategory=&tag=",
        'api_url_mark': "api/v1/search/job/posts",
        'extra_fields': ['location', 'department']
    },
    {
        'id': 3,
        'name': 'ç¤¾ä¼šæ‹›è˜',
        'sheet_name': 'experienced',
        'url': "https://jobs.bytedance.com/experienced/position?keywords=&category=6704215864629004552%2C6704215864591255820%2C6704215924712409352%2C6704216224387041544&location=CT_125&project=&type=&job_hot_flag=&current=1&limit=600&functionCategory=&tag=",
        'api_url_mark': "api/v1/search/job/posts",
        'extra_fields': ['location', 'department']
    }
]

# --- 2. æ ¸å¿ƒé€»è¾‘åŒº ---

class JobMonitor:
    """å­—èŠ‚è·³åŠ¨èŒä½ç›‘æ§å™¨ï¼ˆå¼‚æ­¥ç‰ˆï¼‰ï¼Œå°è£…äº†æŠ“å–ã€æ•°æ®å¤„ç†ã€ä¿å­˜å’Œé€šçŸ¥çš„å…¨éƒ¨é€»è¾‘ã€‚"""

    def __init__(self, tasks: List[Dict[str, Any]], filename: Path, headless: bool = True):
        self.tasks = tasks
        self.filename = filename
        self.headless = headless
        self.results: List[tuple[str, str, List[Dict[str, Any]]]] = []

    @staticmethod
    def _generate_job_hash(job_data: Dict[str, Any]) -> str:
        """ä¸ºèŒä½æ•°æ®ç”Ÿæˆå”¯ä¸€çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºè¯†åˆ«é‡å¤èŒä½ã€‚"""
        key_fields = ['code', 'title', 'description', 'requirement']
        hash_string = ''.join(str(job_data.get(field, '')) for field in key_fields)
        return hashlib.md5(hash_string.encode('utf-8')).hexdigest()

    def _load_existing_hashes(self) -> Dict[str, Set[str]]:
        """ä»ç°æœ‰çš„Excelæ–‡ä»¶ä¸­åŠ è½½æ¯ä¸ªå·¥ä½œè¡¨çš„èŒä½å“ˆå¸Œå€¼ã€‚"""
        existing_hashes: Dict[str, Set[str]] = {}
        if not self.filename.exists():
            logging.info(f"æ–‡ä»¶ {self.filename} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
            return existing_hashes

        try:
            with pd.ExcelFile(self.filename, engine='openpyxl') as xls:
                for sheet_name in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=sheet_name)
                    hashes = {self._generate_job_hash(row.to_dict()) for _, row in df.iterrows()}
                    existing_hashes[sheet_name] = hashes
                    logging.info(f"å·²ä»å·¥ä½œè¡¨ '{sheet_name}' åŠ è½½ {len(hashes)} ä¸ªèŒä½å“ˆå¸Œã€‚")
        except Exception as e:
            logging.warning(f"è¯»å–ç°æœ‰æ–‡ä»¶ {self.filename} å‡ºé”™: {e}ã€‚å°†ä½œä¸ºé¦–æ¬¡è¿è¡Œå¤„ç†ã€‚")
        return existing_hashes

    async def _run_single_task_async(self, task_config: Dict[str, Any], browser: Browser) -> None:
        """åœ¨ç‹¬ç«‹çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸­å¼‚æ­¥è¿è¡Œå•ä¸ªæŠ“å–ä»»åŠ¡ã€‚"""
        task_name = task_config['name']
        sheet_name = task_config['sheet_name']
        scraped_jobs: List[Dict[str, Any]] = []
        context = None
        
        try:
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            logging.info(f"ğŸš€ å¼€å§‹ä»»åŠ¡: {task_name}")

            async with page.expect_response(lambda r: task_config['api_url_mark'] in r.url, timeout=30000) as response_info:
                await page.goto(task_config['url'], wait_until="domcontentloaded")
            
            response = await response_info.value
            if response.status == 200:
                data = await response.json()
                job_list = data.get("data", {}).get("job_post_list", [])
                
                for job in job_list:
                    publish_time = datetime.fromtimestamp(job["publish_time"] / 1000)
                    job_info = {
                        "title": job.get("title"),
                        "description": job.get("description"),
                        "requirement": job.get("requirement"),
                        "publish_time": publish_time.strftime("%Y-%m-%d %H:%M:%S"),
                        "code": job.get("code"),
                    }
                    for field in task_config['extra_fields']:
                        value = job.get(field)
                        job_info[field] = value.get('name') if isinstance(value, dict) else value
                    scraped_jobs.append(job_info)
                
                logging.info(f"âœ… ä»»åŠ¡ '{task_name}' æˆåŠŸè·å– {len(scraped_jobs)} ä¸ªèŒä½ã€‚")
            else:
                logging.error(f"âŒ ä»»åŠ¡ '{task_name}' API å“åº”çŠ¶æ€ç : {response.status}")

        except Exception as e:
            logging.error(f"âŒ ä»»åŠ¡ '{task_name}' æ‰§è¡Œå¤±è´¥: {e}", exc_info=False)
        finally:
            if context:
                await context.close()
            self.results.append((sheet_name, task_name, scraped_jobs))

    def _process_results(self, existing_hashes: Dict[str, Set[str]]) -> Dict:
        """å¤„ç†æ‰€æœ‰ä»»åŠ¡ç»“æœï¼Œåˆå¹¶æ•°æ®å¹¶è¯†åˆ«æ–°èŒä½ã€‚"""
        final_data_frames: Dict[str, pd.DataFrame] = {}
        summary_info: List[Dict[str, Any]] = []
        
        for sheet_name, task_name, new_jobs_data in self.results:
            if not new_jobs_data:
                summary_info.append({'task_name': task_name, 'new_count': 0, 'total_count': 0})
                continue
            
            previous_hashes = existing_hashes.get(sheet_name, set())
            
            for job in new_jobs_data:
                job_hash = self._generate_job_hash(job)
                job['is_new'] = job_hash not in previous_hashes
                
            df = pd.DataFrame(new_jobs_data)
            df = df.sort_values(by="publish_time", ascending=False)
            
            final_data_frames[sheet_name] = df
            summary_info.append({
                'task_name': task_name,
                'new_count': df['is_new'].sum(),
                'total_count': len(df)
            })
        return {"data_frames": final_data_frames, "summary": summary_info}

    def _save_and_highlight(self, data_frames: Dict[str, pd.DataFrame]) -> None:
        """å°†æ•°æ®ä¿å­˜åˆ°Excelï¼Œå¹¶ä¸ºæ–°èŒä½è¡Œåº”ç”¨é«˜äº®ã€‚"""
        if not data_frames:
            logging.info("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜ã€‚")
            return
            
        try:
            with pd.ExcelWriter(self.filename, engine='openpyxl') as writer:
                for sheet_name, df in data_frames.items():
                    display_df = df.drop(columns=['is_new'], errors='ignore')
                    display_df.to_excel(writer, sheet_name=sheet_name, index=False)
            
            workbook = openpyxl.load_workbook(self.filename)
            highlight_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
            
            for sheet_name, df in data_frames.items():
                if sheet_name in workbook.sheetnames and 'is_new' in df.columns:
                    worksheet = workbook[sheet_name]
                    for row_idx, is_new in enumerate(df['is_new'], start=2):
                        if is_new:
                            for col_idx in range(1, worksheet.max_column + 1):
                                worksheet.cell(row=row_idx, column=col_idx).fill = highlight_fill
            
            workbook.save(self.filename)
            logging.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜å¹¶é«˜äº®è‡³: {self.filename}")

        except Exception as e:
            logging.error(f"âš ï¸ ä¿å­˜æˆ–é«˜äº®Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")

    @staticmethod
    def _send_notification(summary: List[Dict[str, Any]]) -> None:
        """æ ¹æ®æ“ä½œç³»ç»Ÿå‘é€æ¡Œé¢é€šçŸ¥ï¼Œå¢å¼ºäº†ç¨³å®šæ€§å’Œé”™è¯¯æ’æŸ¥èƒ½åŠ›ã€‚"""
        total_new = sum(info.get('new_count', 0) for info in summary)
        total_count = sum(info.get('total_count', 0) for info in summary)
        current_time = datetime.now().strftime("%H:%M")

        if total_new > 0:
            title = "ğŸ‰ å‘ç°æ–°èŒä½!"
            details = "\\n".join(
                f"   â€¢ {info['task_name']}: +{info['new_count']} ä¸ª"
                for info in summary if info.get('new_count', 0) > 0
            )
            message = f"å‘ç° {total_new} ä¸ªæ–°ä¸ç“œï¼\\næ€»è®¡: {total_count} ä¸ªä¸ç“œ\\næ—¶é—´: {current_time}\\n\\nè¯¦æƒ…:\\n{details}"
            buttons = '{"ç¨åæŸ¥çœ‹", "ç«‹å³æŸ¥çœ‹"}'
            default_button = '"ç«‹å³æŸ¥çœ‹"'
            action_script = f'do shell script "open \\"{OUTPUT_FILENAME}\\""'
        else:
            title = "âœ… ç›‘æ§å®Œæˆ"
            message = f"æœ¬æ¬¡æœªå‘ç°æ–°ä¸ç“œã€‚\\næ€»è®¡: {total_count} ä¸ªä¸ç“œ\\næ—¶é—´: {current_time}"
            buttons = '{"ç¡®å®š"}'
            default_button = '"ç¡®å®š"'
            action_script = ""

        if sys.platform == "darwin":
            script = f'''
            try
                set response to display dialog "{message}" with title "{title}" buttons {buttons} default button {default_button} with icon note
                if button returned of response is "ç«‹å³æŸ¥çœ‹" then
                    {action_script}
                end if
            on error errMsg number errNum
                -- æ­¤å¤„ç•™ç©ºï¼Œä»¥ä¾¿åœ¨ç”¨æˆ·å–æ¶ˆæˆ–å¯¹è¯æ¡†è‡ªåŠ¨æ¶ˆå¤±æ—¶é™é»˜å¤„ç†ï¼Œé¿å…Pythonç«¯æŠ¥é”™
            end try
            '''
            try:
                result = subprocess.run(
                    ['osascript', '-e', script], 
                    capture_output=True, text=True, timeout=120, check=False
                )
                if result.returncode != 0:
                    logging.error("âš ï¸ å‘é€ macOS é€šçŸ¥å¤±è´¥! AppleScript æ‰§è¡Œå‡ºé”™ã€‚")
                    logging.error(f"   - è¿”å›ç : {result.returncode}")
                    logging.error(f"   - é”™è¯¯è¾“å‡º: {result.stderr.strip()}")
                    logging.info("   - è¯·å‚è€ƒæ–‡æ¡£ä¸­çš„â€œé€šçŸ¥é—®é¢˜æ’æŸ¥æŒ‡å—â€è¿›è¡Œæ£€æŸ¥ã€‚")
                else:
                    logging.info("ğŸ”” å·²æˆåŠŸè§¦å‘æ¡Œé¢é€šçŸ¥è„šæœ¬ã€‚")
            except subprocess.TimeoutExpired:
                logging.warning("âš ï¸ å‘é€ macOS é€šçŸ¥è¶…æ—¶ã€‚ç”¨æˆ·å¯èƒ½æœªåœ¨2åˆ†é’Ÿå†…å“åº”å¯¹è¯æ¡†ã€‚")
            except Exception as e:
                logging.error(f"âš ï¸ è°ƒç”¨é€šçŸ¥å­è¿›ç¨‹æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        else:
            logging.info("--- æ¡Œé¢é€šçŸ¥ (émacOS) ---")
            logging.info(f"æ ‡é¢˜: {title}")
            logging.info(message.replace("\\n", "\n"))
            logging.info("-------------------------")

    async def run_async(self, silent_mode: bool = False):
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„èŒä½ç›‘æ§æµç¨‹ï¼ˆå¼‚æ­¥ç‰ˆï¼‰ã€‚"""
        start_time = datetime.now()
        logging.info(f"--- å¼€å§‹ç›‘æ§ {start_time.strftime('%Y-%m-%d %H:%M:%S')} ---")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        DOCUMENTS_PATH.mkdir(exist_ok=True)
        
        existing_hashes = self._load_existing_hashes()
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=self.headless)
            tasks_to_run = [self._run_single_task_async(task, browser) for task in self.tasks]
            await asyncio.gather(*tasks_to_run)
            await browser.close()

        results = self._process_results(existing_hashes)
        data_frames = results["data_frames"]
        summary = results["summary"]
        
        self._save_and_highlight(data_frames)
        
        total_new = sum(info.get('new_count', 0) for info in summary)
        if not silent_mode or total_new > 0:
            logging.info("--- ç›‘æ§ç»“æœ ---")
            for info in summary:
                logging.info(f"  - {info['task_name']}: å‘ç° {info.get('new_count', 0)} ä¸ªæ–°ä¸ç“œï¼Œå…± {info.get('total_count', 0)} ä¸ªã€‚")
            logging.info(f"æ€»è®¡æ–°å¢: {total_new} ä¸ª")
        
        self._send_notification(summary)
        
        end_time = datetime.now()
        logging.info(f"--- ç›‘æ§ç»“æŸ, è€—æ—¶: {(end_time - start_time).total_seconds():.2f} ç§’ ---")

# --- 3. ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    try:
        is_silent = "--auto" in sys.argv
        monitor = JobMonitor(tasks=TASK_CONFIGS, filename=OUTPUT_FILENAME, headless=True)
        asyncio.run(monitor.run_async(silent_mode=is_silent))
        
    except KeyboardInterrupt:
        logging.info("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
        sys.exit(0)
    except Exception as e:
        if "Executable doesn't exist" in str(e):
            logging.critical("âŒ Playwright æµè§ˆå™¨ä¾èµ–æœªå®‰è£…! è¯·è¿è¡Œ 'playwright install' å‘½ä»¤ã€‚")
        else:
            logging.critical(f"\nâŒ ç¨‹åºå‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)
