# bytedance_job_monitor.py

import asyncio
import hashlib
import json
import logging
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Any

import openpyxl
import pandas as pd
from openpyxl.styles import PatternFill
from playwright.async_api import async_playwright, Browser

# --- 1. é…ç½®åŒº ---

# æ—¥å¿—é…ç½®# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)

# æ–‡ä»¶ååŠè·¯å¾„é…ç½®
# æ–‡ä»¶å°†ä¿å­˜åœ¨å½“å‰ç”¨æˆ·çš„"æ–‡ç¨¿" (Documents) æ–‡ä»¶å¤¹ä¸‹
DOCUMENTS_PATH = Path.home() / "Documents"
OUTPUT_FILENAME = DOCUMENTS_PATH / "bytedance_jobs_tracker.xlsx"
JSON_CACHE_FILENAME = DOCUMENTS_PATH / "bytedance_jobs_cache.json"

# ä»»åŠ¡é…ç½®
TASK_CONFIGS: List[Dict[str, Any]] = [
    {
        'id': 1,
        'name': 'å®ä¹ æ‹›è˜',
        'sheet_name': 'intern',
        'url': "https://jobs.bytedance.com/campus/position?keywords=&category=6704215864629004552%2C6704215864591255820%2C6704216224387041544%2C6704215924712409352&location=CT_125&project=7481474995534301447%2C7468181472685164808%2C7194661644654577981%2C7194661126919358757&type=&job_hot_flag=&current=1&limit=2000&functionCategory=&tag=",
        'api_url_mark': "api/v1/search/job/posts",
        'extra_fields': []
    },
    {
        'id': 2,
        'name': 'æ ¡å›­æ‹›è˜',
        'sheet_name': 'campus',
        'url': "https://jobs.bytedance.com/campus/position?keywords=&category=6704215864629004552%2C6704215864591255820%2C6704216224387041544%2C6704215924712409352&location=CT_125&project=7525009396952582407&type=&job_hot_flag=&current=1&limit=2000&functionCategory=&tag=",
        'api_url_mark': "api/v1/search/job/posts",
        'extra_fields': ['location', 'department']
    },
    {
        'id': 3,
        'name': 'ç¤¾ä¼šæ‹›è˜',
        'sheet_name': 'experienced',
        'url': "https://jobs.bytedance.com/experienced/position?keywords=&category=6704215864629004552%2C6704215864591255820%2C6704215924712409352%2C6704216224387041544&location=CT_125&project=&type=&job_hot_flag=&current=1&limit=600&functionCategory=&tag=",
        'api_url_mark': "api/v1/search/job/posts",
        'extra_fields': ['location', 'department']
    }
]

# --- 2. æ ¸å¿ƒé€»è¾‘åŒº ---

class JobMonitor:
    """å­—èŠ‚è·³åŠ¨èŒä½ç›‘æ§å™¨ï¼ˆå¼‚æ­¥ç‰ˆï¼‰ï¼Œå°è£…äº†æŠ“å–ã€æ•°æ®å¤„ç†ã€ä¿å­˜å’Œé€šçŸ¥çš„å…¨éƒ¨é€»è¾‘ã€‚"""

    def __init__(self, tasks: List[Dict[str, Any]], filename: Path, headless: bool = True):
        self.tasks = tasks
        self.filename = filename
        self.json_cache_filename = JSON_CACHE_FILENAME
        self.headless = headless
        self.results: List[tuple[str, str, List[Dict[str, Any]]]] = []

    @staticmethod
    def _generate_job_hash(job_data: Dict[str, Any]) -> str:
        """ä¸ºèŒä½æ•°æ®ç”Ÿæˆå”¯ä¸€çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºè¯†åˆ«é‡å¤èŒä½ã€‚"""
        key_fields = ['code', 'title', 'description', 'requirement']
        hash_string = ''.join(str(job_data.get(field, '')) for field in key_fields)
        return hashlib.md5(hash_string.encode('utf-8')).hexdigest()
    
    def _save_json_cache(self, data_frames: Dict[str, pd.DataFrame]) -> None:
        """å°†æ•°æ®ä¿å­˜ä¸ºJSONç¼“å­˜æ–‡ä»¶ã€‚"""
        try:
            cache_data = {}
            for sheet_name, df in data_frames.items():
                # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼Œå¤„ç†æ—¶é—´æ ¼å¼
                records = df.to_dict('records')
                for record in records:
                    # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
                    for key, value in record.items():
                        if pd.isna(value):
                            record[key] = None
                        elif isinstance(value, (pd.Timestamp, datetime)):
                            record[key] = str(value)
                cache_data[sheet_name] = records
            
            with open(self.json_cache_filename, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            logging.info(f"ğŸ’¾ JSONç¼“å­˜å·²ä¿å­˜è‡³: {self.json_cache_filename}")
        except Exception as e:
            logging.error(f"âš ï¸ ä¿å­˜JSONç¼“å­˜æ—¶å‡ºé”™: {e}")
    
    def _load_json_cache(self) -> Dict[str, pd.DataFrame]:
        """ä»JSONç¼“å­˜æ–‡ä»¶åŠ è½½æ•°æ®ã€‚"""
        cache_dataframes: Dict[str, pd.DataFrame] = {}
        
        if not self.json_cache_filename.exists():
            logging.info(f"JSONç¼“å­˜æ–‡ä»¶ {self.json_cache_filename} ä¸å­˜åœ¨ã€‚")
            return cache_dataframes
        
        try:
            with open(self.json_cache_filename, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            for sheet_name, records in cache_data.items():
                if records:
                    df = pd.DataFrame(records)
                    # æ¸…ç©ºæ—§çš„é«˜äº®æ ‡è®°
                    if 'highlight_time' in df.columns:
                        df['highlight_time'] = None
                    cache_dataframes[sheet_name] = df
                    logging.info(f"å·²ä»JSONç¼“å­˜åŠ è½½å·¥ä½œè¡¨ '{sheet_name}' çš„ {len(records)} æ¡è®°å½•ã€‚")
            
        except Exception as e:
            logging.warning(f"è¯»å–JSONç¼“å­˜æ–‡ä»¶ {self.json_cache_filename} å‡ºé”™: {e}ã€‚å°†å¿½ç•¥ç¼“å­˜ã€‚")
        
        return cache_dataframes
    
    @staticmethod
    def _sort_jobs_dataframe(df: pd.DataFrame) -> pd.DataFrame:
        """ç»Ÿä¸€çš„å²—ä½æ’åºæ–¹æ³•ï¼šæ–°å²—ä½(is_new=True)æ’åœ¨æœ€å‰ï¼Œç„¶åæŒ‰å‘å¸ƒæ—¶é—´é™åºæ’åˆ—ã€‚"""
        if 'is_new' in df.columns:
            # å…ˆæŒ‰is_newé™åº(Trueåœ¨å‰)ï¼Œå†æŒ‰publish_timeé™åº
            df = df.sort_values(by=['is_new', 'publish_time'], ascending=[False, False])
        else:
            # å¦‚æœæ²¡æœ‰is_newå­—æ®µï¼ŒåªæŒ‰å‘å¸ƒæ—¶é—´æ’åº
            df = df.sort_values(by='publish_time', ascending=False)
        return df

    def _load_existing_hashes(self) -> tuple[Dict[str, Set[str]], Dict[str, pd.DataFrame]]:
        """ä»JSONç¼“å­˜æˆ–Excelæ–‡ä»¶ä¸­åŠ è½½æ¯ä¸ªå·¥ä½œè¡¨çš„èŒä½å“ˆå¸Œå€¼å’Œæ•°æ®æ¡†ã€‚"""
        existing_hashes: Dict[str, Set[str]] = {}
        existing_dataframes: Dict[str, pd.DataFrame] = {}
        
        # ä¼˜å…ˆä»JSONç¼“å­˜åŠ è½½
        cache_dataframes = self._load_json_cache()
        if cache_dataframes:
            logging.info("ä½¿ç”¨JSONç¼“å­˜æ•°æ®ã€‚")
            existing_dataframes = cache_dataframes
        elif self.filename.exists():
            logging.info("JSONç¼“å­˜ä¸å­˜åœ¨ï¼Œä»Excelæ–‡ä»¶åŠ è½½æ•°æ®ã€‚")
            try:
                with pd.ExcelFile(self.filename, engine='openpyxl') as xls:
                    for sheet_name in xls.sheet_names:
                        df = pd.read_excel(xls, sheet_name=sheet_name)
                        
                        # å»é™¤æ—§çš„é«˜äº®æ ‡è®°ï¼šæ¸…ç©ºhighlight_timeå­—æ®µ
                        if 'highlight_time' in df.columns:
                            df['highlight_time'] = None
                        
                        existing_dataframes[sheet_name] = df
                        logging.info(f"å·²ä»Excelå·¥ä½œè¡¨ '{sheet_name}' åŠ è½½ {len(df)} æ¡è®°å½•ã€‚")
            except Exception as e:
                logging.warning(f"è¯»å–Excelæ–‡ä»¶ {self.filename} å‡ºé”™: {e}ã€‚å°†ä½œä¸ºé¦–æ¬¡è¿è¡Œå¤„ç†ã€‚")
        else:
            logging.info(f"Excelæ–‡ä»¶ {self.filename} å’ŒJSONç¼“å­˜éƒ½ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
        
        # ä¸ºæ‰€æœ‰æ•°æ®æ¡†ç”Ÿæˆå“ˆå¸Œå€¼
        for sheet_name, df in existing_dataframes.items():
            hashes = {self._generate_job_hash(row.to_dict()) for _, row in df.iterrows()}
            existing_hashes[sheet_name] = hashes
            logging.info(f"å·²ä¸ºå·¥ä½œè¡¨ '{sheet_name}' ç”Ÿæˆ {len(hashes)} ä¸ªèŒä½å“ˆå¸Œã€‚")
        
        return existing_hashes, existing_dataframes

    async def _run_single_task_async(self, task_config: Dict[str, Any], browser: Browser) -> None:
        """åœ¨ç‹¬ç«‹çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸­å¼‚æ­¥è¿è¡Œå•ä¸ªæŠ“å–ä»»åŠ¡ã€‚"""
        task_name = task_config['name']
        sheet_name = task_config['sheet_name']
        scraped_jobs: List[Dict[str, Any]] = []
        context = None
        
        try:
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            logging.info(f"ğŸš€ å¼€å§‹ä»»åŠ¡: {task_name}")

            async with page.expect_response(lambda r: task_config['api_url_mark'] in r.url, timeout=30000) as response_info:
                await page.goto(task_config['url'], wait_until="domcontentloaded")
            
            response = await response_info.value
            if response.status == 200:
                data = await response.json()
                job_list = data.get("data", {}).get("job_post_list", [])
                
                # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ªèŒä½çš„å®Œæ•´æ•°æ®ç»“æ„
                if job_list and logging.getLogger().isEnabledFor(logging.DEBUG):
                    logging.debug(f"APIè¿”å›çš„ç¬¬ä¸€ä¸ªèŒä½å®Œæ•´æ•°æ®: {json.dumps(job_list[0], ensure_ascii=False, indent=2)}")
                
                for job in job_list:
                    publish_time = datetime.fromtimestamp(job["publish_time"] / 1000)
                    
                    # æ‰©å±•job_infoï¼ŒåŒ…å«æ›´å¤šAPIå­—æ®µ
                    job_info = {
                        # åŸºç¡€ä¿¡æ¯
                        "title": job.get("title"),
                        "sub_title": job.get("sub_title"),
                        "description": job.get("description"),
                        "requirement": job.get("requirement"),
                        "publish_time": publish_time.strftime("%Y-%m-%d %H:%M:%S"),
                        "code": job.get("code"),
                        
                        # èŒä½åŸºæœ¬ä¿¡æ¯
                        "job_id": job.get("id"),
                        "job_type": job.get("job_type"),
                        "job_category": job.get("job_category", {}).get("name") if isinstance(job.get("job_category"), dict) else job.get("job_category"),
                        "job_function": job.get("job_function", {}).get("name") if isinstance(job.get("job_function"), dict) else job.get("job_function"),
                        "department_id": job.get("department_id"),
                        "job_process_id": job.get("job_process_id"),
                        
                        # æ‹›è˜ç±»å‹å’Œé¡¹ç›®ä¿¡æ¯
                        "recruit_type_name": job.get("recruit_type", {}).get("name") if isinstance(job.get("recruit_type"), dict) else None,
                        "recruit_type_parent": job.get("recruit_type", {}).get("parent", {}).get("name") if isinstance(job.get("recruit_type"), dict) and job.get("recruit_type", {}).get("parent") else None,
                        "job_subject_name": job.get("job_subject", {}).get("name", {}).get("zh_cn") if isinstance(job.get("job_subject"), dict) and isinstance(job.get("job_subject", {}).get("name"), dict) else job.get("job_subject", {}).get("name") if isinstance(job.get("job_subject"), dict) else None,
                        
                        # åœ°ç†ä½ç½®ä¿¡æ¯
                        "city_list": ", ".join([city.get("name", "") for city in job.get("city_list", []) if isinstance(city, dict)]) if job.get("city_list") else None,
                        "city_codes": ", ".join([city.get("code", "") for city in job.get("city_list", []) if isinstance(city, dict)]) if job.get("city_list") else None,
                        "address": job.get("address"),
                        
                        # èŒä½è¦æ±‚
                        "degree": job.get("degree"),
                        "experience": job.get("experience"),
                        "min_salary": job.get("min_salary"),
                        "max_salary": job.get("max_salary"),
                        "currency": job.get("currency"),
                        "head_count": job.get("head_count"),
                        
                        # èŒä½çŠ¶æ€å’Œæ ‡è¯†
                        "job_hot_flag": job.get("job_hot_flag"),
                        "is_urgent": job.get("is_urgent"),
                        "job_active_status": job.get("job_active_status"),
                        "recommend_id": job.get("recommend_id"),
                        
                        # å…¶ä»–ä¿¡æ¯
                        "team_name": job.get("team_name"),
                        "brand_name": job.get("brand_name"),
                        "ats_online_apply": job.get("ats_online_apply"),
                        "pc_job_url": job.get("pc_job_url"),
                        "wap_job_url": job.get("wap_job_url"),
                        "storefront_mode": job.get("storefront_mode"),
                        "process_type": job.get("process_type"),
                    }
                    
                    # å¤„ç†é…ç½®ä¸­çš„é¢å¤–å­—æ®µ
                    for field in task_config['extra_fields']:
                        value = job.get(field)
                        job_info[field] = value.get('name') if isinstance(value, dict) else value
                    
                    # æ¸…ç†Noneå€¼ï¼Œä¿æŒæ•°æ®æ•´æ´
                    job_info = {k: v for k, v in job_info.items() if v is not None and v != ''}
                    
                    scraped_jobs.append(job_info)
                
                logging.info(f"âœ… ä»»åŠ¡ '{task_name}' æˆåŠŸè·å– {len(scraped_jobs)} ä¸ªèŒä½ã€‚")
            else:
                logging.error(f"âŒ ä»»åŠ¡ '{task_name}' API å“åº”çŠ¶æ€ç : {response.status}")

        except Exception as e:
            logging.error(f"âŒ ä»»åŠ¡ '{task_name}' æ‰§è¡Œå¤±è´¥: {e}", exc_info=False)
        finally:
            if context:
                await context.close()
            self.results.append((sheet_name, task_name, scraped_jobs))

    def _process_results(self, existing_hashes: Dict[str, Set[str]], existing_dataframes: Dict[str, pd.DataFrame]) -> Dict:
        """å¤„ç†æ‰€æœ‰ä»»åŠ¡ç»“æœï¼Œåˆå¹¶æ•°æ®å¹¶è¯†åˆ«æ–°èŒä½ã€‚"""
        final_data_frames: Dict[str, pd.DataFrame] = {}
        summary_info: List[Dict[str, Any]] = []
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        for sheet_name, task_name, new_jobs_data in self.results:
            previous_hashes = existing_hashes.get(sheet_name, set())
            existing_df = existing_dataframes.get(sheet_name, pd.DataFrame())
            
            if not new_jobs_data:
                # å¦‚æœæ²¡æœ‰æ–°æ•°æ®ï¼Œä½†æœ‰æ—§æ•°æ®ï¼Œåˆ™ä¿ç•™æ—§æ•°æ®
                if not existing_df.empty:
                    final_data_frames[sheet_name] = self._sort_jobs_dataframe(existing_df)
                summary_info.append({'task_name': task_name, 'new_count': 0, 'total_count': len(existing_df)})
                continue
            
            # å¤„ç†æ–°æŠ“å–çš„æ•°æ®
            for job in new_jobs_data:
                job_hash = self._generate_job_hash(job)
                is_new = job_hash not in previous_hashes
                job['is_new'] = is_new
                # ä¸ºæ–°å²—ä½æ·»åŠ é«˜äº®æ—¶é—´æ ‡è®°
                job['highlight_time'] = current_time if is_new else None
            
            new_df = pd.DataFrame(new_jobs_data)
            
            # åˆå¹¶æ–°æ—§æ•°æ®
            if not existing_df.empty:
                # ç¡®ä¿æ—§æ•°æ®æœ‰å¿…è¦çš„å­—æ®µ
                if 'is_new' not in existing_df.columns:
                    existing_df['is_new'] = False
                if 'highlight_time' not in existing_df.columns:
                    existing_df['highlight_time'] = None
                
                # åˆå¹¶æ•°æ®å¹¶å»é‡ï¼ˆåŸºäºå“ˆå¸Œå€¼ï¼‰
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                # æ ¹æ®èŒä½å“ˆå¸Œå»é‡ï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
                combined_df['job_hash'] = combined_df.apply(lambda row: self._generate_job_hash(row.to_dict()), axis=1)
                combined_df = combined_df.drop_duplicates(subset=['job_hash'], keep='last')
                combined_df = combined_df.drop(columns=['job_hash'])
                final_df = combined_df
            else:
                final_df = new_df
            
            final_df = self._sort_jobs_dataframe(final_df)
            final_data_frames[sheet_name] = final_df
            
            new_count = final_df['is_new'].sum() if 'is_new' in final_df.columns else 0
            summary_info.append({
                'task_name': task_name,
                'new_count': new_count,
                'total_count': len(final_df)
            })
        
        return {"data_frames": final_data_frames, "summary": summary_info}

    @staticmethod
    def _extract_team_intro(description: str) -> str:
        """ä»descriptionä¸­æå–å›¢é˜Ÿä»‹ç»éƒ¨åˆ†ã€‚"""
        if not description:
            return ""
        
        # æŸ¥æ‰¾å›¢é˜Ÿä»‹ç»çš„ç»“æŸä½ç½®
        text = description.strip()
        
        import re
        
        # å¦‚æœç›´æ¥ä»¥æ•°å­—ç¼–å·å¼€å§‹ï¼Œè¯´æ˜æ²¡æœ‰å›¢é˜Ÿä»‹ç»
        if re.match(r'^[1-9]ã€', text):
            return "æš‚æ— å›¢é˜Ÿä»‹ç»"
        
        # å¯»æ‰¾å›¢é˜Ÿä»‹ç»çš„ç»“æŸæ ‡å¿—
        patterns = [
            r'å›¢é˜Ÿä»‹ç»[ï¼š:][^\n]*\n\n',  # å›¢é˜Ÿä»‹ç»ï¼š...\n\n
            r'å›¢é˜Ÿä»‹ç»[ï¼š:][^1-9]*(?=[1-9]ã€)',  # å›¢é˜Ÿä»‹ç»ï¼š...ç›´åˆ°æ•°å­—ç¼–å·å¼€å§‹
            r'^[^1-9]*?(?=[1-9]ã€)',  # ä»å¼€å¤´åˆ°ç¬¬ä¸€ä¸ªæ•°å­—ç¼–å·
            r'^.*?(?=\n\n[1-9])',  # ä»å¼€å¤´åˆ°åŒæ¢è¡Œ+æ•°å­—
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.MULTILINE | re.DOTALL)
            if match:
                result = match.group(0).strip()
                return result if result else "æš‚æ— å›¢é˜Ÿä»‹ç»"
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„åˆ†å‰²ç‚¹ï¼Œå–å‰é¢çš„éƒ¨åˆ†
        lines = text.split('\n')
        if len(lines) > 3:
            # å¯»æ‰¾ç¬¬ä¸€ä¸ªä»¥æ•°å­—å¼€å¤´çš„è¡Œ
            for i, line in enumerate(lines):
                if re.match(r'^[1-9]ã€', line.strip()):
                    intro = '\n'.join(lines[:i]).strip()
                    return intro if intro else "æš‚æ— å›¢é˜Ÿä»‹ç»"
        
        # é»˜è®¤è¿”å›å‰200ä¸ªå­—ç¬¦
        if len(text) > 200:
            return text[:200] + '...'
        else:
            return text if text else "æš‚æ— å›¢é˜Ÿä»‹ç»"
    
    @staticmethod
    def _extract_job_details(description: str) -> str:
        """ä»descriptionä¸­æå–å²—ä½ç»†èŠ‚éƒ¨åˆ†ã€‚"""
        if not description:
            return ""
        
        text = description.strip()
        
        # å¯»æ‰¾å²—ä½ç»†èŠ‚çš„å¼€å§‹ä½ç½®
        import re
        patterns = [
            r'([1-9]ã€.*)',  # ä»ç¬¬ä¸€ä¸ªæ•°å­—ç¼–å·å¼€å§‹
            r'(èŒè´£[ï¼š:].*)',  # ä»èŒè´£å¼€å§‹
            r'(å·¥ä½œå†…å®¹[ï¼š:].*)',  # ä»å·¥ä½œå†…å®¹å¼€å§‹
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.MULTILINE | re.DOTALL)
            if match:
                return match.group(1).strip()
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„å¼€å§‹ç‚¹ï¼Œå°è¯•ä»å›¢é˜Ÿä»‹ç»åå¼€å§‹
        team_intro = JobMonitor._extract_team_intro(description)
        if team_intro and len(team_intro) < len(text):
            remaining = text[len(team_intro):].strip()
            return remaining if remaining else text
        
        # é»˜è®¤è¿”å›åé¢çš„éƒ¨åˆ†
        return text[200:] if len(text) > 200 else text

    def _save_and_highlight(self, data_frames: Dict[str, pd.DataFrame]) -> None:
        """å°†æ•°æ®ä¿å­˜åˆ°Excelå’ŒJSONç¼“å­˜ï¼Œå¹¶ä¸ºæ–°èŒä½è¡Œåº”ç”¨é«˜äº®ã€‚
        Excelåªä¿å­˜æŒ‡å®šå­—æ®µï¼ŒJSONä¿ç•™å…¨éƒ¨å­—æ®µã€‚"""
        if not data_frames:
            logging.info("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜ã€‚")
            return
            
        try:
            # ä¸ºExcelåˆ›å»ºç®€åŒ–çš„æ•°æ®æ¡†
            excel_data_frames = {}
            for sheet_name, df in data_frames.items():
                # åˆ›å»ºExcelä¸“ç”¨çš„æ•°æ®æ¡†ï¼ŒåªåŒ…å«æŒ‡å®šå­—æ®µ
                excel_df = df.copy()
                
                # æ‹†åˆ†descriptionå­—æ®µä¸ºå›¢é˜Ÿä»‹ç»å’Œå²—ä½ç»†èŠ‚
                excel_df['å›¢é˜Ÿä»‹ç»'] = excel_df['description'].apply(self._extract_team_intro)
                excel_df['å²—ä½ç»†èŠ‚'] = excel_df['description'].apply(self._extract_job_details)
                
                # æ·»åŠ èŒä½é“¾æ¥
                excel_df['èŒä½é“¾æ¥'] = excel_df['job_id'].apply(
                    lambda x: f"https://jobs.bytedance.com/campus/position/{x}/detail" if pd.notna(x) else ""
                )
                
                # é€‰æ‹©Exceléœ€è¦çš„å­—æ®µ
                excel_columns = [
                    'title', 'å›¢é˜Ÿä»‹ç»', 'å²—ä½ç»†èŠ‚', 'requirement', 
                    'publish_time', 'code', 'èŒä½é“¾æ¥', 'highlight_time'
                ]
                
                # åªä¿ç•™å­˜åœ¨çš„åˆ—
                available_columns = [col for col in excel_columns if col in excel_df.columns]
                excel_df = excel_df[available_columns]
                
                # é‡å‘½ååˆ—ä¸ºä¸­æ–‡
                column_mapping = {
                    'title': 'èŒä½æ ‡é¢˜',
                    'requirement': 'èŒä½è¦æ±‚', 
                    'publish_time': 'å‘å¸ƒæ—¶é—´',
                    'code': 'èŒä½ç¼–å·',
                    'highlight_time': 'æ›´æ–°æ—¶é—´'
                }
                excel_df = excel_df.rename(columns=column_mapping)
                
                excel_data_frames[sheet_name] = excel_df
            
            # ä¿å­˜Excelæ–‡ä»¶
            with pd.ExcelWriter(self.filename, engine='openpyxl') as writer:
                for sheet_name, df in excel_data_frames.items():
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
            
            # åº”ç”¨Excelé«˜äº®
            workbook = openpyxl.load_workbook(self.filename)
            highlight_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
            
            highlight_stats = {}
            for sheet_name, df in excel_data_frames.items():
                if sheet_name in workbook.sheetnames:
                    worksheet = workbook[sheet_name]
                    new_count = 0
                    
                    # é«˜äº®æ–°å¢èŒä½ï¼ˆæ›´æ–°æ—¶é—´ä¸ä¸ºç©ºçš„è¡Œï¼‰
                    for idx, row in df.iterrows():
                        if pd.notna(row.get('æ›´æ–°æ—¶é—´')):
                            excel_row = idx + 2  # Excelè¡Œå·ä»1å¼€å§‹ï¼ŒåŠ ä¸Šè¡¨å¤´
                            for col in range(1, len(df.columns) + 1):
                                worksheet.cell(row=excel_row, column=col).fill = highlight_fill
                            new_count += 1
                    
                    highlight_stats[sheet_name] = new_count
            
            workbook.save(self.filename)
            logging.info(f"ğŸ’¾ Excelæ•°æ®å·²ä¿å­˜å¹¶é«˜äº®è‡³: {self.filename}")
            
            # è®°å½•é«˜äº®ç»Ÿè®¡ä¿¡æ¯
            for sheet_name, count in highlight_stats.items():
                if count > 0:
                    logging.info(f"   ğŸ“Œ å·¥ä½œè¡¨ '{sheet_name}' é«˜äº®äº† {count} ä¸ªæ–°èŒä½")
            
            # ä¿å­˜JSONç¼“å­˜ï¼ˆä¿ç•™å…¨éƒ¨å­—æ®µï¼‰
            self._save_json_cache(data_frames)

        except Exception as e:
            logging.error(f"âš ï¸ ä¿å­˜Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            # å³ä½¿Excelä¿å­˜å¤±è´¥ï¼Œä¹Ÿå°è¯•ä¿å­˜JSONç¼“å­˜
            try:
                self._save_json_cache(data_frames)
            except Exception as cache_error:
                logging.error(f"âš ï¸ ä¿å­˜JSONç¼“å­˜æ—¶å‡ºé”™: {cache_error}")

    @staticmethod
    def _send_notification(summary: List[Dict[str, Any]]) -> None:
        """æ ¹æ®æ“ä½œç³»ç»Ÿå‘é€æ¡Œé¢é€šçŸ¥ï¼Œå¢å¼ºäº†ç¨³å®šæ€§å’Œé”™è¯¯æ’æŸ¥èƒ½åŠ›ã€‚"""
        total_new = sum(info.get('new_count', 0) for info in summary)
        total_count = sum(info.get('total_count', 0) for info in summary)
        current_time = datetime.now().strftime("%H:%M")

        if total_new > 0:
            title = "ğŸ‰ å‘ç°æ–°èŒä½!"
            details = "\\n".join(
                f"   â€¢ {info['task_name']}: +{info['new_count']} ä¸ª"
                for info in summary if info.get('new_count', 0) > 0
            )
            message = f"å‘ç° {total_new} ä¸ªæ–°ä¸ç“œï¼\\næ€»è®¡: {total_count} ä¸ªä¸ç“œ\\næ—¶é—´: {current_time}\\n\\nè¯¦æƒ…:\\n{details}"
            buttons = '{"ç¨åæŸ¥çœ‹", "ç«‹å³æŸ¥çœ‹"}'
            default_button = '"ç«‹å³æŸ¥çœ‹"'
            action_script = f'do shell script "open \\"{OUTPUT_FILENAME}\\""'
        else:
            title = "âœ… ç›‘æ§å®Œæˆ"
            message = f"æœ¬æ¬¡æœªå‘ç°æ–°ä¸ç“œã€‚\\næ€»è®¡: {total_count} ä¸ªä¸ç“œ\\næ—¶é—´: {current_time}"
            buttons = '{"ç¡®å®š"}'
            default_button = '"ç¡®å®š"'
            action_script = ""

        if sys.platform == "darwin":
            script = f'''
            try
                set response to display dialog "{message}" with title "{title}" buttons {buttons} default button {default_button} with icon note
                if button returned of response is "ç«‹å³æŸ¥çœ‹" then
                    {action_script}
                end if
            on error errMsg number errNum
                -- æ­¤å¤„ç•™ç©ºï¼Œä»¥ä¾¿åœ¨ç”¨æˆ·å–æ¶ˆæˆ–å¯¹è¯æ¡†è‡ªåŠ¨æ¶ˆå¤±æ—¶é™é»˜å¤„ç†ï¼Œé¿å…Pythonç«¯æŠ¥é”™
            end try
            '''
            try:
                result = subprocess.run(
                    ['osascript', '-e', script], 
                    capture_output=True, text=True, timeout=120, check=False
                )
                if result.returncode != 0:
                    logging.error("âš ï¸ å‘é€ macOS é€šçŸ¥å¤±è´¥! AppleScript æ‰§è¡Œå‡ºé”™ã€‚")
                    logging.error(f"   - è¿”å›ç : {result.returncode}")
                    logging.error(f"   - é”™è¯¯è¾“å‡º: {result.stderr.strip()}")
                    logging.info("   - è¯·å‚è€ƒæ–‡æ¡£ä¸­çš„â€œé€šçŸ¥é—®é¢˜æ’æŸ¥æŒ‡å—â€è¿›è¡Œæ£€æŸ¥ã€‚")
                else:
                    logging.info("ğŸ”” å·²æˆåŠŸè§¦å‘æ¡Œé¢é€šçŸ¥è„šæœ¬ã€‚")
            except subprocess.TimeoutExpired:
                logging.warning("âš ï¸ å‘é€ macOS é€šçŸ¥è¶…æ—¶ã€‚ç”¨æˆ·å¯èƒ½æœªåœ¨2åˆ†é’Ÿå†…å“åº”å¯¹è¯æ¡†ã€‚")
            except Exception as e:
                logging.error(f"âš ï¸ è°ƒç”¨é€šçŸ¥å­è¿›ç¨‹æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        else:
            logging.info("--- æ¡Œé¢é€šçŸ¥ (émacOS) ---")
            logging.info(f"æ ‡é¢˜: {title}")
            logging.info(message.replace("\\n", "\n"))
            logging.info("-------------------------")

    async def run_async(self, silent_mode: bool = False):
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„èŒä½ç›‘æ§æµç¨‹ï¼ˆå¼‚æ­¥ç‰ˆï¼‰ã€‚"""
        start_time = datetime.now()
        logging.info(f"--- å¼€å§‹ç›‘æ§ {start_time.strftime('%Y-%m-%d %H:%M:%S')} ---")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        DOCUMENTS_PATH.mkdir(exist_ok=True)
        
        existing_hashes, existing_dataframes = self._load_existing_hashes()
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=self.headless)
            tasks_to_run = [self._run_single_task_async(task, browser) for task in self.tasks]
            await asyncio.gather(*tasks_to_run)
            await browser.close()

        results = self._process_results(existing_hashes, existing_dataframes)
        data_frames = results["data_frames"]
        summary = results["summary"]
        
        self._save_and_highlight(data_frames)
        
        total_new = sum(info.get('new_count', 0) for info in summary)
        if not silent_mode or total_new > 0:
            logging.info("--- ç›‘æ§ç»“æœ ---")
            for info in summary:
                logging.info(f"  - {info['task_name']}: å‘ç° {info.get('new_count', 0)} ä¸ªæ–°ä¸ç“œï¼Œå…± {info.get('total_count', 0)} ä¸ªã€‚")
            logging.info(f"æ€»è®¡æ–°å¢: {total_new} ä¸ª")
        
        self._send_notification(summary)
        
        end_time = datetime.now()
        logging.info(f"--- ç›‘æ§ç»“æŸ, è€—æ—¶: {(end_time - start_time).total_seconds():.2f} ç§’ ---")

# --- 3. ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    try:
        is_silent = "--auto" in sys.argv
        monitor = JobMonitor(tasks=TASK_CONFIGS, filename=OUTPUT_FILENAME, headless=True)
        asyncio.run(monitor.run_async(silent_mode=is_silent))
        
    except KeyboardInterrupt:
        logging.info("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
        sys.exit(0)
    except Exception as e:
        if "Executable doesn't exist" in str(e):
            logging.critical("âŒ Playwright æµè§ˆå™¨ä¾èµ–æœªå®‰è£…! è¯·è¿è¡Œ 'playwright install' å‘½ä»¤ã€‚")
        else:
            logging.critical(f"\nâŒ ç¨‹åºå‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)
